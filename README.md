# Au-delÃ  des Heuristiques Statiques : Conception et Mise en Å’uvre d'un Agent d'Apprentissage par Renforcement pour l'Optimisation Holistique et Dynamique de la Logistique MiniÃ¨re

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11.11](https://img.shields.io/badge/python-3.11.11-blue.svg)](https://www.python.org/downloads/)
[![Anaconda](https://img.shields.io/badge/Anaconda-%2344A833.svg?style=flat&logo=anaconda&logoColor=white)](https://www.anaconda.com/)
<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

**MÃ©moire de Master 2 â€” Apprentissage par Renforcement appliquÃ© Ã  la Logistique MiniÃ¨re**

PrÃ©sentÃ© par **Jean-Luc Mupasa Kalunga**  
SupervisÃ© par **[Nom du directeur de mÃ©moire]**  
Co-dirigÃ© par **[Nom du co-directeur]** *(si applicable)*

UniversitÃ© **[Nom de l'universitÃ©]** â€” AnnÃ©e acadÃ©mique 2024-2025

---

## Table des MatiÃ¨res

- [Introduction](#introduction)
- [ProblÃ©matique](#problÃ©matique)
- [HypothÃ¨se](#hypothÃ¨se)
- [Questions de Recherche](#questions-de-recherche)
- [RÃ©sumÃ© du Projet](#rÃ©sumÃ©-du-projet)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [MÃ©thodologie](#mÃ©thodologie)
- [RÃ©sultats Attendus](#rÃ©sultats-attendus)
- [RÃ©fÃ©rences ClÃ©s](#rÃ©fÃ©rences-clÃ©s)
- [Structure du Projet](#structure-du-projet)
- [Licence](#licence)
- [Contact](#contact)

---

## Introduction

Dans un contexte minier oÃ¹ l'optimisation des coÃ»ts opÃ©rationnels et la rÃ©duction de l'empreinte environnementale sont devenues des impÃ©ratifs stratÃ©giques, les systÃ¨mes de gestion de flotte (Fleet Management Systems - FMS) traditionnels montrent leurs limites. Bien que des solutions industrielles comme **Caterpillar MineStar Dispatch** et **Komatsu Jigsaw** aient rÃ©volutionnÃ© la coordination des flottes, ces systÃ¨mes reposent principalement sur des heuristiques statiques, des rÃ¨gles prÃ©dÃ©finies et des zones tampons pour gÃ©rer les flux de vÃ©hicules.

Ces approches, bien qu'efficaces dans des conditions stables, peinent Ã  s'adapter dynamiquement aux **micro-conditions** Ã©volutives :
- Ã‰tat dÃ©gradÃ© des routes (nids-de-poule, boue, pentes variables)
- Variations stochastiques du trafic aux points nÃ©vralgiques
- Conditions mÃ©tÃ©orologiques imprÃ©visibles
- Style de conduite et Ã©tat mÃ©canique des vÃ©hicules

Ce projet propose de **dÃ©passer ces limites** en dÃ©veloppant un systÃ¨me d'aide Ã  la dÃ©cision basÃ© sur l'**Apprentissage par Renforcement (Reinforcement Learning - RL)**, capable d'apprendre une politique de conduite holistique optimisant simultanÃ©ment l'itinÃ©raire ET le rythme de dÃ©placement des vÃ©hicules.

---

## ProblÃ©matique

Les FMS actuels optimisent localement (affectation camion â†’ pelle, gestion des files d'attente), mais ne parviennent pas Ã  :
1. **Optimiser globalement** l'ensemble de la flotte en temps rÃ©el.
2. **Anticiper et s'adapter** aux perturbations dynamiques (dÃ©gradation route, congestion imprÃ©vue).
3. **Minimiser le temps d'attente improductif** (moteur tournant au ralenti dans les zones tampons).
4. **Apprendre continuellement** des donnÃ©es opÃ©rationnelles pour amÃ©liorer les dÃ©cisions futures.

**Constat critique** : Un camion suivant une rÃ¨gle statique peut sprinter vers sa destination pour finalement attendre dans une zone tampon, gaspillant carburant et temps. Une approche adaptative pourrait ajuster le rythme de dÃ©placement pour arriver "juste Ã  temps".

---

## HypothÃ¨se

**L'utilisation d'un agent autonome basÃ© sur l'Apprentissage par Renforcement (RL) permettra de dÃ©passer les performances des systÃ¨mes heuristiques actuels en apprenant une politique de navigation adaptative qui :**
- Minimise la consommation de carburant par tonne-kilomÃ¨tre
- RÃ©duit drastiquement le temps d'attente improductif
- S'adapte en temps rÃ©el aux conditions changeantes de l'environnement minier

Cette hypothÃ¨se sera validÃ©e par une comparaison rigoureuse entre l'agent RL et des baselines reprÃ©sentatives des approches actuelles (heuristiques statiques + logique "Dispatch" simplifiÃ©e).

---

## Questions de Recherche

1. **Comment modÃ©liser l'environnement minier complexe** (rÃ©seau routier, zones tampons, Ã©vÃ©nements stochastiques) dans un cadre RL compatible avec les standards industriels (Gymnasium/OpenAI Gym) ?

2. **Quelle architecture d'agent RL** (DQN, PPO, SAC, etc.) est la plus adaptÃ©e pour apprendre une politique de navigation holistique intÃ©grant Ã  la fois le choix d'itinÃ©raire et le contrÃ´le de vitesse ?

3. **Comment concevoir une fonction de rÃ©compense** qui capture fidÃ¨lement les coÃ»ts rÃ©els (carburant, usure, temps) tout en pÃ©nalisant les comportements non souhaitables (attentes prolongÃ©es, accÃ©lÃ©rations brutales) ?

4. **Quelles mÃ©triques et protocoles d'Ã©valuation** permettront de dÃ©montrer quantitativement la supÃ©rioritÃ© de l'approche RL par rapport aux heuristiques statiques et aux systÃ¨mes "Dispatch" actuels, notamment dans des scÃ©narios de perturbation (dÃ©gradation route, pic de trafic) ?

5. **Comment intÃ©grer un tel systÃ¨me RL** comme couche d'intelligence additionnelle dans l'Ã©cosystÃ¨me des FMS existants, sans nÃ©cessiter un remplacement complet de l'infrastructure ?

---

## RÃ©sumÃ© du Projet

Ce projet vise Ã  dÃ©velopper un **agent d'Apprentissage par Renforcement (RL)** capable d'optimiser de maniÃ¨re holistique et dynamique la logistique d'une flotte de camions miniers. Contrairement aux approches traditionnelles qui appliquent des rÃ¨gles fixes (ex : "toujours prendre le chemin le plus court"), notre agent apprendra une politique adaptative qui :

- **Choisit l'itinÃ©raire optimal** en fonction de l'Ã©tat actuel du rÃ©seau routier, du trafic et de l'occupation des destinations.
- **Ajuste le rythme de dÃ©placement** (vitesse cible) pour minimiser la consommation tout en Ã©vitant les attentes inutiles.
- **S'adapte en temps rÃ©el** aux perturbations (dÃ©gradation route, congestion, mÃ©tÃ©o).

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Environnement de Simulation               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RÃ©seau    â”‚  â”‚  VÃ©hicules   â”‚  â”‚  Ã‰vÃ©nements      â”‚   â”‚
â”‚  â”‚  Routier   â”‚  â”‚  (Flotte)    â”‚  â”‚  Dynamiques      â”‚   â”‚
â”‚  â”‚  (Graphe)  â”‚  â”‚              â”‚  â”‚  (Stochastiques) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ Observations
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Agent RL (PPO/DQN)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Policy Network : State â†’ (ItinÃ©raire, Vitesse)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ Actions
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Fonction de RÃ©compense                    â”‚
â”‚  R = -Î±Â·Fuel - Î²Â·Time - Î³Â·WaitTime - Î´Â·Wear               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison avec les Approches Existantes

| CaractÃ©ristique              | Heuristiques Statiques (ex: Dijkstra) | FMS Actuels (Dispatch) | Agent RL (Notre Approche) |
|------------------------------|----------------------------------------|------------------------|----------------------------|
| Choix d'itinÃ©raire           | Plus court chemin fixe                 | Plus court + gestion files | Adaptatif (Ã©tat rÃ©seau)   |
| ContrÃ´le de vitesse          | âŒ Non                                  | âš ï¸ LimitÃ© (rÃ¨gles)      | âœ… Oui (optimal dynamique) |
| Adaptation aux perturbations | âŒ Non                                  | âš ï¸ RÃ©active (re-routing) | âœ… Anticipative (apprentissage) |
| Minimisation attentes        | âŒ Non                                  | âš ï¸ Zones tampons        | âœ… Synchronisation prÃ©dictive |
| Apprentissage continu        | âŒ Non                                  | âŒ Non                  | âœ… Oui (amÃ©lioration continue) |

---

## Installation

### PrÃ©requis

- Python 3.11.11 (via Anaconda)
- Anaconda ou Miniconda
- Git
- (Optionnel) CUDA pour GPU (entraÃ®nement accÃ©lÃ©rÃ©)

### Ã‰tapes

1. **Cloner le dÃ©pÃ´t**
   ```bash
   git clone https://github.com/[ton-username]/memoire-master-rl-logistique.git
   cd memoire-master-rl-logistique
   ```

2. **CrÃ©er un environnement conda**
   ```bash
   conda create -n datascience python=3.11.11
   conda activate datascience
   ```

3. **Installer les dÃ©pendances**
   ```bash
   pip install -r requirements.txt
   # ou avec conda
   conda install --file requirements.txt
   ```

4. **Installer le package en mode dÃ©veloppement**
   ```bash
   pip install -e .
   ```

### DÃ©pendances Principales

- `gymnasium` : Environnements RL standardisÃ©s
- `stable-baselines3` : ImplÃ©mentations RL (PPO, DQN, SAC)
- `torch` : Deep Learning backend
- `networkx` : Manipulation de graphes (rÃ©seau routier)
- `pandas`, `numpy` : Traitement de donnÃ©es
- `matplotlib`, `seaborn` : Visualisation
- `pytest` : Tests unitaires

---

## Utilisation

### 1. Lancer une Simulation de Baseline (Dijkstra)

```bash
python -m memoire_master_rl_logistique.baselines.dijkstra --scenario=default
```

### 2. EntraÃ®ner un Agent RL (PPO)

```bash
python -m memoire_master_rl_logistique.agents.ppo_agent --train \
    --timesteps=1000000 \
    --save-path=models/ppo_mine_agent
```

### 3. Ã‰valuer et Comparer les ModÃ¨les

```bash
python -m memoire_master_rl_logistique.evaluate \
    --models ppo,dqn,dijkstra,dispatch \
    --scenarios perturbation_route,pic_trafic \
    --output reports/comparison.csv
```

### 4. Visualiser les RÃ©sultats

```bash
python -m memoire_master_rl_logistique.visualize \
    --input reports/comparison.csv \
    --output reports/figures/
```

### 5. Lancer l'Interface de DÃ©monstration (Streamlit)

```bash
streamlit run app.py
```

---

## MÃ©thodologie

Le projet suit la mÃ©thodologie **CRISP-DM (Cross-Industry Standard Process for Data Mining)** adaptÃ©e au Machine Learning et au Reinforcement Learning :

### Phase 1 : ComprÃ©hension MÃ©tier et DonnÃ©es (Business & Data Understanding)
- **Revue de littÃ©rature approfondie** :
  - Ã‰tude des FMS existants (Caterpillar MineStar, Komatsu Jigsaw)
  - Application du RL au Vehicle Routing Problem (VRP)
  - ModÃ¨les de consommation de carburant pour engins lourds
- **DÃ©finition des KPIs** :
  - Consommation (L/tÂ·km)
  - Temps de cycle moyen (min)
  - Temps d'attente improductif (min)
  - Taux d'utilisation des assets (%)
- **Analyse des contraintes rÃ©elles** : zones tampons, capacitÃ©s de chargement, rÃ¨gles de sÃ©curitÃ©

### Phase 2 : PrÃ©paration des DonnÃ©es et Simulation
- **ModÃ©lisation du rÃ©seau routier** : Graphe orientÃ© pondÃ©rÃ© (NetworkX) avec attributs dynamiques
- **CrÃ©ation de l'environnement de simulation** (Gymnasium) :
  - Espace d'Ã©tat : `(position, destination, charge, trafic_local, Ã©tat_route, occupation_cibles)`
  - Espace d'action : `(prochain_noeud, vitesse_cible)`
  - Fonction de rÃ©compense : `R = -Î±Â·Fuel - Î²Â·Time - Î³Â·WaitTime - Î´Â·Wear`
- **ModÃ©lisation des Ã©vÃ©nements stochastiques** :
  - DÃ©gradation progressive des routes
  - Variations alÃ©atoires de trafic
  - Conditions mÃ©tÃ©orologiques

### Phase 3 : ModÃ©lisation (Modeling)
- **ImplÃ©mentation de l'agent RL** :
  - Algorithmes testÃ©s : PPO, DQN, SAC (Stable-Baselines3)
  - Architecture rÃ©seau : MLP (Multi-Layer Perceptron) ou CNN si grille spatiale
- **DÃ©veloppement des baselines** :
  - Baseline 1 : Dijkstra (plus court chemin statique)
  - Baseline 2 : Heuristique "Dispatch" (simulation FMS actuel avec zones tampons)
- **EntraÃ®nement et tuning** : Hyperparameter search (Optuna), curriculum learning

### Phase 4 : Ã‰valuation (Evaluation)
- **ScÃ©narios de test** :
  - ScÃ©nario nominal (conditions stables)
  - ScÃ©nario de perturbation (dÃ©gradation route soudaine)
  - ScÃ©nario de pic de trafic
  - ScÃ©nario mÃ©tÃ©o adverse
- **MÃ©triques comparatives** :
  - Fuel consumption (L), Time (min), Wait time (min), Wear cost (â‚¬)
- **Analyses statistiques** : Tests de significativitÃ© (t-test, ANOVA), intervalles de confiance

### Phase 5 : DÃ©ploiement (Deployment - Conceptuel)
- **Prototype de dÃ©monstration** (Streamlit/Gradio)
- **IntÃ©gration conceptuelle** : Proposition d'architecture pour intÃ©gration dans FMS existant
- **Documentation** : Rapport de mÃ©moire, prÃ©sentation de soutenance

---

## RÃ©sultats Attendus

Ã€ l'issue de ce projet, nous anticipons :

1. **Validation quantitative** : DÃ©monstration d'une rÃ©duction de 10-25% de la consommation de carburant et 30-50% du temps d'attente par rapport aux baselines statiques.

2. **Preuve de concept robuste** : Un environnement de simulation rÃ©aliste et un agent RL entraÃ®nÃ© capable de gÃ©nÃ©raliser Ã  divers scÃ©narios de perturbation.

3. **Contribution scientifique** : Publication des rÃ©sultats dans une confÃ©rence/journal (ex: IEEE Intelligent Transportation Systems, Applied Energy).

4. **Recommandations industrielles** : Proposition d'une architecture d'intÃ©gration de l'agent RL comme module additionnel aux FMS actuels.

5. **Code open-source** : Mise Ã  disposition du code (sous licence MIT) pour reproductibilitÃ© et extension par la communautÃ©.

---

## RÃ©fÃ©rences ClÃ©s

### SystÃ¨mes de Gestion de Flotte MiniÃ¨re
- Caterpillar. (2023). *MineStar System Architecture*. Technical Documentation.
- Komatsu. (2022). *Jigsaw Fleet Management System*. Product Whitepaper.

### Apprentissage par Renforcement pour la Logistique
- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press.
- Nazari, M., et al. (2018). "Reinforcement Learning for Solving the Vehicle Routing Problem". *NeurIPS*.

### Optimisation de la Consommation de Carburant
- Alarie, S., & Gamache, M. (2002). "Overview of Solution Strategies Used in Truck Dispatching Systems for Open Pit Mines". *International Journal of Surface Mining*.

### ModÃ©lisation et Simulation
- OpenAI. (2023). *Gymnasium: A Standard API for Reinforcement Learning*. Documentation.

*(Voir `references/` pour la liste complÃ¨te des articles PDF)*

---

## Structure du Projet

Ce projet suit la structure **Cookiecutter Data Science** pour garantir reproductibilitÃ© et maintenabilitÃ© :

```
memoire-master-rl-logistique/
â”‚
â”œâ”€â”€ LICENSE                          <- Licence MIT
â”œâ”€â”€ Makefile                         <- Commandes automatisÃ©es (make train, make test)
â”œâ”€â”€ README.md                        <- Ce fichier
â”œâ”€â”€ pyproject.toml                   <- Configuration projet (Poetry/setuptools)
â”œâ”€â”€ requirements.txt                 <- DÃ©pendances Python
â”œâ”€â”€ setup.cfg                        <- Configuration flake8, pytest
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ external/                    <- DonnÃ©es tierces (cartes, benchmarks)
â”‚   â”œâ”€â”€ interim/                     <- DonnÃ©es intermÃ©diaires (graphes traitÃ©s)
â”‚   â”œâ”€â”€ processed/                   <- DonnÃ©es finales prÃªtes pour entraÃ®nement
â”‚   â””â”€â”€ raw/                         <- DonnÃ©es brutes originales
â”‚
â”œâ”€â”€ docs/                            <- Documentation (MkDocs)
â”‚   â”œâ”€â”€ mkdocs.yml
â”‚   â”œâ”€â”€ getting-started.md
â”‚   â””â”€â”€ index.md
â”‚
â”œâ”€â”€ memoire_master_rl_logistique/    <- Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                    <- Variables et configuration globales
â”‚   â”œâ”€â”€ dataset.py                   <- Scripts de gÃ©nÃ©ration de donnÃ©es
â”‚   â”œâ”€â”€ features.py                  <- Feature engineering
â”‚   â”‚
â”‚   â”œâ”€â”€ envs/                        <- Environnements de simulation (Gymnasium)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mine_env.py              <- Environnement principal
â”‚   â”‚   â””â”€â”€ road_network.py          <- ModÃ©lisation du graphe routier
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                      <- ImplÃ©mentation des agents RL
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py             <- Agent PPO (Stable-Baselines3)
â”‚   â”‚   â””â”€â”€ dqn_agent.py             <- Agent DQN
â”‚   â”‚
â”‚   â”œâ”€â”€ baselines/                   <- ModÃ¨les de rÃ©fÃ©rence
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dijkstra.py              <- Plus court chemin statique
â”‚   â”‚   â””â”€â”€ dispatch_heuristic.py    <- Simulation FMS actuel
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      <- ModÃ¨les physiques (consommation, usure)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ fuel_model.py            <- ModÃ¨le de consommation de carburant
â”‚   â”‚
â”‚   â”œâ”€â”€ modeling/                    <- Pipeline d'entraÃ®nement et prÃ©diction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py                 <- Code d'entraÃ®nement
â”‚   â”‚   â””â”€â”€ predict.py               <- InfÃ©rence avec modÃ¨les entraÃ®nÃ©s
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       <- Utilitaires (visualisation, mÃ©triques)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ plots.py                 <- Code de visualisation
â”‚       â””â”€â”€ metrics.py               <- Calcul des KPIs
â”‚
â”œâ”€â”€ models/                          <- ModÃ¨les entraÃ®nÃ©s sauvegardÃ©s
â”‚   â”œâ”€â”€ ppo_mine_agent.zip
â”‚   â””â”€â”€ dqn_mine_agent.zip
â”‚
â”œâ”€â”€ notebooks/                       <- Notebooks Jupyter (CRISP-DM)
â”‚   â”œâ”€â”€ 1.0-business-and-data-understanding.ipynb
â”‚   â”œâ”€â”€ 2.0-exploratory-data-analysis.ipynb
â”‚   â”œâ”€â”€ 3.0-modeling.ipynb
â”‚   â””â”€â”€ 4.0-evaluation.ipynb
â”‚
â”œâ”€â”€ references/                      <- Articles scientifiques, documentation FMS
â”‚   â”œâ”€â”€ Alarie_Gamache_2002.pdf
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ reports/                         <- Rapports gÃ©nÃ©rÃ©s, figures pour le mÃ©moire
â”‚   â”œâ”€â”€ figures/
â”‚   â”‚   â”œâ”€â”€ comparison_fuel.png
â”‚   â”‚   â””â”€â”€ comparison_waittime.png
â”‚   â””â”€â”€ final_report.pdf
â”‚
â””â”€â”€ tests/                           <- Tests unitaires
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_env.py
    â””â”€â”€ test_agents.py
```

---

## Contributions

Les contributions sont bienvenues ! Si vous souhaitez :
- Signaler un bug â†’ Ouvrir une [issue](https://github.com/[ton-username]/memoire-master-rl-logistique/issues)
- Proposer une amÃ©lioration â†’ CrÃ©er une [pull request](https://github.com/[ton-username]/memoire-master-rl-logistique/pulls)
- Discuter du projet â†’ [Discussions](https://github.com/[ton-username]/memoire-master-rl-logistique/discussions)

---

## Licence

Ce projet est sous licence **MIT**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

Vous Ãªtes libre de :
- âœ… Utiliser ce code pour vos propres recherches
- âœ… Modifier et distribuer
- âœ… Utiliser Ã  des fins commerciales

Ã€ condition de :
- ğŸ“„ Inclure une copie de la licence et du copyright
- ğŸ™ Citer ce travail dans vos publications

### Citation

Si vous utilisez ce code dans vos recherches, merci de citer :

```bibtex
@mastersthesis{mupasa2025rl-mining,
  author  = {Jean-Luc Mupasa Kalunga},
  title   = {Au-delÃ  des Heuristiques Statiques : Conception et Mise en Å’uvre 
             d'un Agent d'Apprentissage par Renforcement pour l'Optimisation 
             Holistique et Dynamique de la Logistique MiniÃ¨re},
  school  = {[Nom de l'universitÃ©]},
  year    = {2025},
  type    = {MÃ©moire de Master},
  url     = {https://github.com/[ton-username]/memoire-master-rl-logistique}
}
```

---

## Contact

**Jean-Luc Mupasa Kalunga**  
ğŸ“§ Email : [0997699393jm@gmail.com](mailto:0997699393jm@gmail.com)  
ğŸ”— GitHub : [@[ton-username]](https://github.com/[ton-username])  
ğŸ“ UniversitÃ© : [Nom de l'universitÃ©]

Pour toute question concernant ce projet, n'hÃ©sitez pas Ã  me contacter ou Ã  ouvrir une issue sur GitHub.

---

## Remerciements

- **[Nom du directeur]** pour l'encadrement et les conseils avisÃ©s
- **[Nom du co-directeur]** pour son expertise technique
- **CommunautÃ© Stable-Baselines3** pour l'excellent framework RL
- **Auteurs des articles de rÃ©fÃ©rence** listÃ©s dans `references/`
- **CollÃ¨gues du Master** pour les discussions enrichissantes

---

<p align="center">
  <i>Ce projet est dÃ©veloppÃ© dans le cadre d'un mÃ©moire de Master 2 en [SpÃ©cialitÃ©].<br>
  DerniÃ¨re mise Ã  jour : Octobre 2025</i>
</p>

<p align="center">
  <a href="#table-des-matiÃ¨res">â¬† Retour en haut</a>
</p>

--------

